This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
docs/
  tech-specs.md
  user-manual.md
src/
  repo_distiller.py
tests/
  .gitkeep
.gitignore
config.yaml
LICENSE
pyproject.toml
README.md
requirements.txt
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="src/repo_distiller.py">
#!/usr/bin/env python3
"""
Repository Distiller - Intelligent repository filtering for LLM context preparation.

This utility creates a filtered copy of a source repository, optimized for providing
context to Large Language Models in AI-assisted coding workflows.

Usage:
    python src/repo_distiller.py <source_dir> <destination_dir> [options]
    # Or from project root:
    uv run python src/repo_distiller.py <source_dir> <destination_dir> [options]

Author: Generated via AI-assisted spec-driven development
Version: 1.0.1 (Path resolution fix)
License: MIT
"""

import argparse
import logging
import sys
import shutil
import json
import csv
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Set, Tuple
from dataclasses import dataclass, field
from enum import Enum

try:
    import yaml
except ImportError:
    print("ERROR: PyYAML is required. Install it with: uv add pyyaml", file=sys.stderr)
    sys.exit(1)


# ============================================================================
# CONFIGURATION & DATA STRUCTURES
# ============================================================================

class FilterAction(Enum):
    """Enumeration of possible actions for a file."""
    COPY = "COPY"
    SAMPLE = "SAMPLE"
    SKIP = "SKIP"


@dataclass
class FilterStats:
    """Statistics tracking for the distillation process."""
    scanned: int = 0
    copied: int = 0
    sampled: int = 0
    skipped: int = 0
    errors: int = 0
    skipped_reasons: Dict[str, int] = field(default_factory=dict)

    def add_skip_reason(self, reason: str):
        """Track skip reasons for summary reporting."""
        self.skipped_reasons[reason] = self.skipped_reasons.get(reason, 0) + 1


@dataclass
class DistillerConfig:
    """Configuration container for the distiller."""
    max_file_size_mb: float
    whitelist_files: List[str]
    whitelist_directories: List[str]
    blacklist_files: List[str]
    blacklist_extensions: List[str]
    blacklist_patterns: List[re.Pattern]
    blacklist_directories: List[str]
    data_sampling_enabled: bool
    data_sampling_extensions: Set[str]
    data_sampling_include_header: bool
    data_sampling_head_rows: int
    data_sampling_tail_rows: int
    ai_coding_env: str = 'chat'

    @staticmethod
    def from_yaml(config_path: Path) -> 'DistillerConfig':
        """Load and parse configuration from YAML file."""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config_data = yaml.safe_load(f)
        except FileNotFoundError:
            raise FileNotFoundError(f"Configuration file not found: {config_path}")
        except yaml.YAMLError as e:
            raise ValueError(f"Invalid YAML in config file: {e}")

        # Parse and compile regex patterns
        patterns = []
        for pattern_str in config_data.get('blacklist', {}).get('patterns', []):
            try:
                patterns.append(re.compile(pattern_str))
            except re.error as e:
                logging.warning(f"Invalid regex pattern '{pattern_str}': {e}")

        # Normalize extensions (ensure leading dot)
        extensions = config_data.get('blacklist', {}).get('extensions', [])
        normalized_exts = [ext if ext.startswith('.') else f'.{ext}' for ext in extensions]

        # Parse data sampling config
        data_sampling = config_data.get('data_sampling', {})
        sampling_exts = data_sampling.get('target_extensions', [])
        normalized_sampling_exts = {ext if ext.startswith('.') else f'.{ext}' for ext in sampling_exts}

        return DistillerConfig(
            max_file_size_mb=config_data.get('max_file_size_mb', 5.0),
            whitelist_files=config_data.get('whitelist', {}).get('files', []),
            whitelist_directories=config_data.get('whitelist', {}).get('directories', []),
            blacklist_files=config_data.get('blacklist', {}).get('files', []),
            blacklist_extensions=normalized_exts,
            blacklist_patterns=patterns,
            blacklist_directories=config_data.get('blacklist', {}).get('directories', []),
            data_sampling_enabled=data_sampling.get('enabled', True),
            data_sampling_extensions=normalized_sampling_exts,
            data_sampling_include_header=data_sampling.get('include_header', True),
            data_sampling_head_rows=data_sampling.get('head_rows', 5),
            data_sampling_tail_rows=data_sampling.get('tail_rows', 5),
            ai_coding_env=config_data.get('ai_coding_env', 'chat')
        )


# ============================================================================
# LOGGING SETUP
# ============================================================================

def setup_logging(log_dir: Path, verbose: bool = False) -> logging.Logger:
    """
    Configure logging with both console and file handlers.

    Args:
        log_dir: Directory for log files
        verbose: If True, set log level to DEBUG

    Returns:
        Configured logger instance
    """
    log_dir.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_dir / f"log_{timestamp}.txt"

    logger = logging.getLogger('repo_distiller')
    logger.setLevel(logging.DEBUG if verbose else logging.INFO)

    # Remove existing handlers to avoid duplicates
    logger.handlers.clear()

    # Console handler - concise format
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.DEBUG if verbose else logging.INFO)
    console_format = logging.Formatter(
        '%(levelname)-8s | %(message)s'
    )
    console_handler.setFormatter(console_format)

    # File handler - detailed format
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(logging.DEBUG)
    file_format = logging.Formatter(
        '%(asctime)s | %(levelname)-8s | %(funcName)-20s | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    file_handler.setFormatter(file_format)

    logger.addHandler(console_handler)
    logger.addHandler(file_handler)

    logger.info(f"Logging initialized. Log file: {log_file}")
    return logger


# ============================================================================
# CORE FILTERING LOGIC
# ============================================================================

class RepositoryDistiller:
    """Main class for repository distillation operations."""

    def __init__(self, config: DistillerConfig, logger: logging.Logger):
        self.config = config
        self.logger = logger
        self.stats = FilterStats()

    def _matches_glob_pattern(self, path: Path, patterns: List[str], base_path: Path) -> bool:
        """
        Check if a path matches any glob pattern in the list.

        Args:
            path: Absolute path to check
            patterns: List of glob patterns
            base_path: Absolute base repository path for relative matching

        Returns:
            True if path matches any pattern
        """
        # Ensure we're working with absolute paths
        path = path.resolve()
        base_path = base_path.resolve()

        try:
            rel_path = path.relative_to(base_path)
        except ValueError:
            self.logger.debug(
                f"Skipping glob checks for path outside repository root: {path}"
            )
            return False
        rel_path_str = str(rel_path)

        for pattern in patterns:
            # Normalize pattern for consistent matching
            pattern_normalized = pattern.strip('./').rstrip('/')

            # Direct match
            if rel_path_str == pattern_normalized:
                return True

            # Directory prefix match (for directory patterns)
            if path.is_dir() and rel_path_str.startswith(pattern_normalized):
                return True

            # Check if file is within a directory pattern
            if '/' in pattern_normalized:
                pattern_parts = Path(pattern_normalized).parts
                rel_parts = rel_path.parts
                if len(rel_parts) >= len(pattern_parts):
                    if rel_parts[:len(pattern_parts)] == pattern_parts:
                        return True

            # Glob pattern matching (supports *)
            if '*' in pattern:
                try:
                    # Use pathlib's match for glob patterns
                    if rel_path.match(pattern_normalized):
                        return True
                except Exception as e:
                    self.logger.warning(f"Invalid glob pattern '{pattern}': {e}")

        return False

    def _is_whitelisted(self, path: Path, base_path: Path) -> bool:
        """Check if path is whitelisted (highest priority)."""
        # File whitelist
        if self._matches_glob_pattern(path, self.config.whitelist_files, base_path):
            self.logger.debug(f"WHITELIST[file]: {path.relative_to(base_path)}")
            return True

        # Directory whitelist
        if self._matches_glob_pattern(path, self.config.whitelist_directories, base_path):
            self.logger.debug(f"WHITELIST[dir]: {path.relative_to(base_path)}")
            return True

        return False

    def _is_blacklisted(self, path: Path, base_path: Path) -> Tuple[bool, Optional[str]]:
        """
        Check if path is blacklisted and return the reason.

        Returns:
            Tuple of (is_blacklisted, reason)
        """
        # File size check (applied after whitelist per requirements)
        if path.is_file():
            size_mb = path.stat().st_size / (1024 * 1024)
            if size_mb > self.config.max_file_size_mb:
                return True, f"file_size>{self.config.max_file_size_mb}MB"

        # Specific file blacklist
        if self._matches_glob_pattern(path, self.config.blacklist_files, base_path):
            return True, "blacklist_file"

        # Directory blacklist
        if self._matches_glob_pattern(path, self.config.blacklist_directories, base_path):
            return True, "blacklist_directory"

        # Extension blacklist
        if path.suffix.lower() in self.config.blacklist_extensions:
            return True, f"blacklist_ext:{path.suffix}"

        # Regex pattern blacklist
        filename = path.name
        for pattern in self.config.blacklist_patterns:
            if pattern.search(filename):
                return True, f"blacklist_pattern:{pattern.pattern}"

        return False, None

    def _should_sample_data_file(self, path: Path) -> bool:
        """Determine if file should be sampled instead of copied verbatim."""
        if not self.config.data_sampling_enabled:
            return False

        if not path.is_file():
            return False

        return path.suffix.lower() in self.config.data_sampling_extensions

    def determine_action(self, path: Path, base_path: Path) -> Tuple[FilterAction, Optional[str]]:
        """
        Determine what action to take for a given path.

        Args:
            path: Absolute path to evaluate
            base_path: Absolute repository base path

        Returns:
            Tuple of (action, skip_reason)
        """
        # Step 1: Whitelist check (highest priority)
        if self._is_whitelisted(path, base_path):
            # Whitelisted files can still be sampled if they match criteria
            if self._should_sample_data_file(path):
                return FilterAction.SAMPLE, None
            return FilterAction.COPY, None

        # Step 2: Blacklist check
        is_blacklisted, reason = self._is_blacklisted(path, base_path)
        if is_blacklisted:
            return FilterAction.SKIP, reason

        # Step 3: Data sampling check
        if self._should_sample_data_file(path):
            return FilterAction.SAMPLE, None

        # Step 4: Default action (not whitelisted = skip in whitelist-only mode)
        return FilterAction.SKIP, "not_whitelisted"

    def _sample_csv_file(self, source: Path, destination: Path) -> bool:
        """
        Sample a CSV file by copying header + head rows + tail rows.

        Returns:
            True if successful, False otherwise
        """
        try:
            # Ensure absolute paths
            source = source.resolve()
            destination = destination.resolve()

            with open(source, 'r', encoding='utf-8', newline='', errors='replace') as src:
                reader = csv.reader(src)

                # Read all rows into memory (for tail access)
                all_rows = list(reader)

                if len(all_rows) == 0:
                    self.logger.warning(f"Empty CSV file: {source}")
                    shutil.copy2(source, destination)
                    return True

                # Determine header
                has_header = self.config.data_sampling_include_header
                header_rows = [all_rows[0]] if has_header and len(all_rows) > 0 else []
                data_start_idx = 1 if has_header else 0

                # Calculate head and tail
                data_rows = all_rows[data_start_idx:]
                total_data_rows = len(data_rows)

                if total_data_rows <= (self.config.data_sampling_head_rows + self.config.data_sampling_tail_rows):
                    # File is small enough, copy all
                    with open(destination, 'w', encoding='utf-8', newline='') as dst:
                        writer = csv.writer(dst)
                        writer.writerows(all_rows)
                else:
                    # Sample head and tail
                    head = data_rows[:self.config.data_sampling_head_rows]
                    tail = data_rows[-self.config.data_sampling_tail_rows:]

                    with open(destination, 'w', encoding='utf-8', newline='') as dst:
                        writer = csv.writer(dst)

                        # Write header
                        if header_rows:
                            writer.writerows(header_rows)

                        # Write head
                        writer.writerows(head)

                        # Write separator comment
                        separator = [f"... ({total_data_rows - len(head) - len(tail)} rows omitted) ..."]
                        writer.writerow(separator)

                        # Write tail
                        writer.writerows(tail)

                self.logger.info(f"SAMPLED[CSV]: {source.name} ({len(all_rows)} rows)")
                return True

        except Exception as e:
            self.logger.error(f"Error sampling CSV {source.name}: {e}")
            return False

    def _sample_json_file(self, source: Path, destination: Path) -> bool:
        """
        Sample a JSON/JSONL file by copying head + tail objects.

        Returns:
            True if successful, False otherwise
        """
        try:
            # Ensure absolute paths
            source = source.resolve()
            destination = destination.resolve()

            with open(source, 'r', encoding='utf-8', errors='replace') as src:
                # Handle JSONL (newline-delimited JSON) by streaming to avoid loading the whole file
                if source.suffix.lower() == '.jsonl':
                    from collections import deque
                    head = []
                    tail = deque(maxlen=self.config.data_sampling_tail_rows)
                    total_objects = 0

                    # Stream lines: collect head (first N non-empty lines) and tail (last M non-empty lines)
                    for raw_line in src:
                        line = raw_line.rstrip('\n\r')
                        if not line.strip():
                            continue
                        total_objects += 1
                        if len(head) < self.config.data_sampling_head_rows:
                            head.append(line)
                        tail.append(line)

                    # Empty file -> copy as-is
                    if total_objects == 0:
                        self.logger.warning(f"Empty JSONL file: {source}")
                        shutil.copy2(source, destination)
                        return True

                    # If small enough, copy everything
                    if total_objects <= (self.config.data_sampling_head_rows + self.config.data_sampling_tail_rows):
                        shutil.copy2(source, destination)
                    else:
                        omitted = total_objects - len(head) - len(tail)
                        with open(destination, 'w', encoding='utf-8') as dst:
                            if head:
                                dst.write('\n'.join(head))
                                dst.write('\n\n')
                            dst.write(f"... ({omitted} objects omitted) ...\n\n")
                            dst.write('\n'.join(list(tail)))

                    self.logger.info(f"SAMPLED[JSONL]: {source.name} ({total_objects} objects)")
                    return True

                # Non-JSONL: read full content (used for regular JSON handling below)
                content = src.read().strip()
 
                # Handle regular JSON (array or object)
                try:
                    data = json.loads(content)
                except json.JSONDecodeError as e:
                    self.logger.warning(f"Invalid JSON in {source.name}: {e}. Copying as-is.")
                    shutil.copy2(source, destination)
                    return True
 
                # If JSON is an array, sample it
                if isinstance(data, list):
                    total_items = len(data)
 
                    head_limit = max(0, int(self.config.data_sampling_head_rows))
                    tail_limit = max(0, int(self.config.data_sampling_tail_rows))
 
                    if total_items <= (head_limit + tail_limit):
                        shutil.copy2(source, destination)
                        self.logger.info(f"SAMPLED[JSON - copied intact]: {source.name} ({total_items} items)")
                        return True
 
                    head = data[:head_limit]
                    tail = data[-tail_limit:] if tail_limit > 0 else []
 
                    sampled = {
                        "_sampled": True,
                        "_total_items": total_items,
                        "_omitted_items": total_items - len(head) - len(tail),
                        "head": head,
                        "tail": tail
                    }
 
                    try:
                        with open(destination, 'w', encoding='utf-8') as dst:
                            json.dump(sampled, dst, indent=2, ensure_ascii=False)
                    except Exception as e:
                        self.logger.error(f"Failed writing sampled JSON to {destination}: {e}")
                        return False
 
                    self.logger.info(f"SAMPLED[JSON]: {source.name} ({total_items} items)")
                    return True
 
                # JSON is an object or primitive: copy-as-is
                shutil.copy2(source, destination)
                self.logger.debug(f"JSON object/primitive (not sampled): {source.name}")
                return True

        except Exception as e:
            self.logger.error(f"Error sampling JSON {source.name}: {e}")
            return False

    def process_file(self, source: Path, destination: Path, action: FilterAction) -> bool:
        """
        Process a single file according to the determined action.

        Args:
            source: Absolute source file path
            destination: Absolute destination file path
            action: Action to perform

        Returns:
            True if successful, False otherwise
        """
        try:
            # Ensure absolute paths
            source = source.resolve()
            destination = destination.resolve()

            destination.parent.mkdir(parents=True, exist_ok=True)

            if action == FilterAction.COPY:
                shutil.copy2(source, destination)
                self.logger.debug(f"COPIED: {source.name}")
                self.stats.copied += 1
                return True

            elif action == FilterAction.SAMPLE:
                # Determine file type and sample accordingly
                ext = source.suffix.lower()

                if ext == '.csv' or ext == '.tsv':
                    success = self._sample_csv_file(source, destination)
                elif ext in {'.json', '.jsonl'}:
                    success = self._sample_json_file(source, destination)
                else:
                    self.logger.warning(f"Unknown data file type for sampling: {ext}. Copying as-is.")
                    shutil.copy2(source, destination)
                    success = True

                if success:
                    self.stats.sampled += 1
                else:
                    self.stats.errors += 1
                return success

            else:  # SKIP
                self.logger.debug(f"SKIPPED: {source.name}")
                self.stats.skipped += 1
                return True

        except Exception as e:
            self.logger.error(f"Error processing {source.name}: {e}")
            self.stats.errors += 1
            return False

    def distill(self, source_dir: Path, dest_dir: Path, dry_run: bool = False) -> bool:
        """
        Main distillation process: walk source directory and filter to destination.

        Args:
            source_dir: Source repository directory (will be resolved to absolute)
            dest_dir: Destination directory (will be resolved to absolute)
            dry_run: If True, only simulate actions without copying

        Returns:
            True if successful, False if errors occurred
        """
        # CRITICAL FIX: Resolve all paths to absolute paths
        source_dir = source_dir.resolve()
        dest_dir = dest_dir.resolve()

        self.logger.info(f"{'[DRY RUN] ' if dry_run else ''}Starting distillation...")
        self.logger.info(f"Source: {source_dir}")
        self.logger.info(f"Destination: {dest_dir}")
        self.logger.info(f"Configuration: AI Coding Env = {self.config.ai_coding_env}")

        if not source_dir.exists():
            self.logger.error(f"Source directory does not exist: {source_dir}")
            return False

        if not source_dir.is_dir():
            self.logger.error(f"Source path is not a directory: {source_dir}")
            return False

        # Prepare destination directory
        if not dry_run:
            if dest_dir.exists():
                if not self._confirm_overwrite(dest_dir):
                    self.logger.info("Operation cancelled by user.")
                    return False
                shutil.rmtree(dest_dir)
            dest_dir.mkdir(parents=True, exist_ok=True)

        # Walk the source directory
        try:
            for path in source_dir.rglob('*'):
                self.stats.scanned += 1

                # Only process files (directories are created as needed)
                if not path.is_file():
                    continue

                # Ensure we're working with absolute paths
                path = path.resolve()

                try:
                    rel_path = path.relative_to(source_dir)
                except ValueError:
                    self.logger.debug(f"Skipping path outside source root: {path}")
                    self.stats.skipped += 1
                    continue

                # Determine action
                action, skip_reason = self.determine_action(path, source_dir)

                # Log and track
                if action == FilterAction.SKIP:
                    if skip_reason:
                        self.stats.add_skip_reason(skip_reason)
                    self.logger.debug(f"SKIP[{skip_reason}]: {rel_path}")
                    self.stats.skipped += 1
                    continue

                # Construct destination path
                dest_path = dest_dir / rel_path

                # Execute action
                if dry_run:
                    self.logger.info(f"[DRY RUN] {action.value}: {rel_path}")
                    if action == FilterAction.COPY:
                        self.stats.copied += 1
                    elif action == FilterAction.SAMPLE:
                        self.stats.sampled += 1
                else:
                    self.process_file(path, dest_path, action)

        except Exception as e:
            self.logger.error(f"Fatal error during distillation: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return False

        # Print summary
        self._print_summary()

        return self.stats.errors == 0

    def _confirm_overwrite(self, dest_dir: Path) -> bool:
        """Prompt user to confirm overwriting existing destination."""
        print(f"\nWARNING: Destination directory exists: {dest_dir}")
        print("All contents will be deleted. Continue? (yes/no): ", end='')
 
        try:
            response = input().strip().lower()
            return response in {'yes', 'y'}
        except (EOFError, KeyboardInterrupt):
            print()
            return False

    def _print_summary(self):
        """Print a summary report of the distillation process."""
        self.logger.info("\n" + "=" * 70)
        self.logger.info("DISTILLATION SUMMARY")
        self.logger.info("=" * 70)
        self.logger.info(f"Total files scanned:  {self.stats.scanned}")
        self.logger.info(f"Files copied:         {self.stats.copied}")
        self.logger.info(f"Files sampled:        {self.stats.sampled}")
        self.logger.info(f"Files skipped:        {self.stats.skipped}")
        self.logger.info(f"Errors:               {self.stats.errors}")
 
        if self.stats.skipped_reasons:
            self.logger.info("\nSkip reasons breakdown:")
            for reason, count in sorted(self.stats.skipped_reasons.items(), key=lambda x: -x[1]):
                self.logger.info(f"  {reason:30s}: {count:>6d}")
 
        self.logger.info("=" * 70 + "\n")


# ============================================================================
# CLI INTERFACE
# ============================================================================

def parse_arguments() -> argparse.Namespace:
    """Parse and validate command-line arguments."""
    parser = argparse.ArgumentParser(
        description="Repository Distiller - Intelligent filtering for LLM context preparation",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic usage (from project root)
  python src/repo_distiller.py ./my-repo ./distilled-repo

  # Or with uv
  uv run python src/repo_distiller.py ./my-repo ./distilled-repo

  # Dry run to preview actions
  python src/repo_distiller.py ./my-repo ./distilled-repo --dry-run

  # Verbose logging with custom config
  python src/repo_distiller.py ./my-repo ./distilled-repo -c custom_config.yaml -v

For more information, see docs/user-manual.md
        """
    )

    parser.add_argument(
        'source_dir',
        type=Path,
        help='Source repository directory to distill'
    )

    parser.add_argument(
        'destination_dir',
        type=Path,
        help='Destination directory for distilled output'
    )

    parser.add_argument(
        '-c', '--config',
        type=Path,
        default=Path('./config.yaml'),
        help='Path to YAML configuration file (default: ./config.yaml)'
    )

    parser.add_argument(
        '-d', '--dry-run',
        action='store_true',
        help='Preview actions without actually copying files'
    )

    parser.add_argument(
        '-v', '--verbose',
        action='store_true',
        help='Enable verbose logging (DEBUG level)'
    )

    parser.add_argument(
        '--log-dir',
        type=Path,
        default=Path('./logs'),
        help='Directory for log files (default: ./logs)'
    )

    parser.add_argument(
        '--version',
        action='version',
        version='Repository Distiller v1.0.1'
    )

    args = parser.parse_args()

    # CRITICAL FIX: Resolve all paths to absolute paths
    args.source_dir = args.source_dir.resolve()
    args.destination_dir = args.destination_dir.resolve()
    args.config = args.config.resolve()
    args.log_dir = args.log_dir.resolve()

    # Validate source directory exists
    if not args.dry_run and not args.source_dir.exists():
        parser.error(f"Source directory does not exist: {args.source_dir}")

    return args


def main() -> int:
    """Main entry point for the repository distiller."""
    args = parse_arguments()

    # Setup logging
    logger = setup_logging(args.log_dir, verbose=args.verbose)

    try:
        # Load configuration
        logger.info(f"Loading configuration from: {args.config}")
        config = DistillerConfig.from_yaml(args.config)
        logger.info(f"Configuration loaded successfully (AI Coding Env: {config.ai_coding_env})")

        # Create distiller instance
        distiller = RepositoryDistiller(config, logger)

        # Execute distillation
        success = distiller.distill(
            source_dir=args.source_dir,
            dest_dir=args.destination_dir,
            dry_run=args.dry_run
        )

        if success:
            logger.info("✓ Distillation completed successfully")
            return 0
        else:
            logger.error("✗ Distillation completed with errors")
            return 1

    except KeyboardInterrupt:
        logger.info("\nOperation cancelled by user")
        return 130  # Standard exit code for Ctrl+C

    except Exception as e:
        logger.exception(f"Fatal error: {e}")
        return 1


if __name__ == '__main__':
    sys.exit(main())
</file>

<file path="tests/.gitkeep">
# This directory is reserved for future test implementation
</file>

<file path=".gitignore">
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[codz]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py.cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# UV
#   Similar to Pipfile.lock, it is generally recommended to include uv.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#uv.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock
#poetry.toml

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#   pdm recommends including project-wide configuration in pdm.toml, but excluding .pdm-python.
#   https://pdm-project.org/en/latest/usage/project/#working-with-version-control
#pdm.lock
#pdm.toml
.pdm-python
.pdm-build/

# pixi
#   Similar to Pipfile.lock, it is generally recommended to include pixi.lock in version control.
#pixi.lock
#   Pixi creates a virtual environment in the .pixi directory, just like venv module creates one
#   in the .venv directory. It is recommended not to include this directory in version control.
.pixi

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.envrc
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

# Abstra
# Abstra is an AI-powered process automation framework.
# Ignore directories containing user credentials, local state, and settings.
# Learn more at https://abstra.io/docs
.abstra/

# Visual Studio Code
#  Visual Studio Code specific template is maintained in a separate VisualStudioCode.gitignore 
#  that can be found at https://github.com/github/gitignore/blob/main/Global/VisualStudioCode.gitignore
#  and can be added to the global gitignore or merged into this file. However, if you prefer, 
#  you could uncomment the following to ignore the entire vscode folder
# .vscode/

# Ruff stuff:
.ruff_cache/

# PyPI configuration file
.pypirc

# Cursor
#  Cursor is an AI-powered code editor. `.cursorignore` specifies files/directories to
#  exclude from AI features like autocomplete and code analysis. Recommended for sensitive data
#  refer to https://docs.cursor.com/context/ignore-files
.cursorignore
.cursorindexingignore

# Marimo
marimo/_static/
marimo/_lsp/
__marimo__/
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2025 Jon Chun

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="pyproject.toml">
[project]
name = "repo-distiller"
version = "1.0.1"
description = "Intelligent repository filtering for LLM context preparation"
authors = [
    {name = "Repository Distiller Project"}
]
readme = "README.md"
license = {text = "MIT"}
requires-python = ">=3.7"
dependencies = [
    "pyyaml>=6.0.1",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "pytest-cov>=4.0.0",
    "black>=23.0.0",
    "mypy>=1.0.0",
    "ruff>=0.1.0",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["src"]

[tool.hatch.build.targets.sdist]
include = [
    "/src",
    "/docs",
    "/tests",
    "/config.yaml",
    "/README.md",
    "/LICENSE",
]

[tool.uv]
dev-dependencies = []

[tool.black]
line-length = 100
target-version = ["py37", "py38", "py39", "py310", "py311"]

[tool.ruff]
line-length = 100
target-version = "py37"

[tool.ruff.lint]
select = ["E", "F", "W", "I", "N", "UP", "ANN", "B", "A", "C4", "DTZ", "T10", "ISC", "ICN", "G", "INP", "PIE", "T20", "PT", "Q", "RSE", "RET", "SLF", "SIM", "TID", "ARG", "PTH", "ERA", "PD", "PL", "TRY", "NPY", "RUF"]
ignore = ["ANN101", "ANN102", "ANN401"]

[tool.mypy]
python_version = "3.7"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
</file>

<file path="requirements.txt">
# Repository Distiller Requirements
# Install with: pip install -r requirements.txt

PyYAML>=6.0.1
</file>

<file path="docs/tech-specs.md">
# Repository Distiller - Technical Specification

**Version:** 1.0.1 (Path Resolution Fix)  
**Updated:** 2025-12-14  
**Module Location:** src/repo_distiller.py

## Version History

### v1.0.1 (2025-12-14) - Path Resolution Fix
**Critical Bug Fix:**
- **Issue:** Using relative paths like `../project-name` caused errors:
  ```
  '../project-name/file.py' is not in the subpath of '/current/working/dir'
  ```
- **Root Cause:** Paths weren't resolved to absolute paths before `Path.relative_to()` operations
- **Fix:** Added `.resolve()` calls to convert all paths to absolute:
  - In `parse_arguments()`: resolve source_dir, destination_dir, config, log_dir
  - In `distill()`: resolve source_dir and dest_dir at method entry
  - In path operations: ensure absolute paths before relative_to()

**Changed Methods:**
- `parse_arguments()` - Added path resolution for all arguments
- `distill()` - Resolves paths to absolute at entry point
- `_matches_glob_pattern()` - Added resolve() calls with comments
- `_sample_csv_file()` - Added resolve() calls
- `_sample_json_file()` - Added resolve() calls
- `process_file()` - Added resolve() calls

### v1.0.0 (2025-12-14)
- Initial release

## System Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                CLI Interface (main)                          │
│  - Argument parsing (argparse)                              │
│  - Path resolution (NEW in v1.0.1)                          │
│  - Logging setup                                            │
└─────────────────────────┬───────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────┐
│          DistillerConfig (from YAML)                         │
│  - Configuration loading                                    │
│  - Validation and normalization                             │
│  - Regex pattern compilation                                │
└─────────────────────────┬───────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────┐
│      RepositoryDistiller (Core Logic)                        │
│  - File system traversal                                    │
│  - Rule application (whitelist → blacklist → sampling)      │
│  - File processing (copy/sample/skip)                       │
│  - Path resolution (NEW in v1.0.1)                          │
│  - Statistics tracking                                      │
└─────────────────────────────────────────────────────────────┘
```

## Critical Path Resolution Implementation

### Problem Statement
Python's `Path.relative_to()` requires both paths to be in the same resolution state (both relative or both absolute). Using relative paths like `../project` with `Path.cwd()` caused:

```
ValueError: '../project/file.py' is not in the subpath of '/absolute/path'
```

### Solution
All paths are resolved to absolute paths using `.resolve()` before any relative path operations:

```python
# In parse_arguments():
args.source_dir = args.source_dir.resolve()
args.destination_dir = args.destination_dir.resolve()
args.config = args.config.resolve()
args.log_dir = args.log_dir.resolve()

# In distill():
source_dir = source_dir.resolve()
dest_dir = dest_dir.resolve()

# In _matches_glob_pattern():
path = path.resolve()
base_path = base_path.resolve()
rel_path = path.relative_to(base_path)  # Now works!
```

## Key Classes

### FilterAction (Enum)
```python
class FilterAction(Enum):
    COPY = "COPY"      # Copy file verbatim
    SAMPLE = "SAMPLE"  # Sample data file (head + tail)
    SKIP = "SKIP"      # Skip file entirely
```

### FilterStats (Dataclass)
```python
@dataclass
class FilterStats:
    scanned: int = 0
    copied: int = 0
    sampled: int = 0
    skipped: int = 0
    errors: int = 0
    skipped_reasons: Dict[str, int] = field(default_factory=dict)
```

### RepositoryDistiller (Main Engine)

**Public Methods:**
- `distill(source_dir, dest_dir, dry_run)` - Main entry point (v1.0.1: resolves paths)
- `determine_action(path, base_path)` - Apply filtering rules
- `process_file(source, destination, action)` - Execute action (v1.0.1: resolves paths)

**Private Methods:**
- `_matches_glob_pattern()` - Glob pattern matching (v1.0.1: resolves paths)
- `_is_whitelisted()` - Whitelist check
- `_is_blacklisted()` - Blacklist check with reason
- `_should_sample_data_file()` - Data sampling check
- `_sample_csv_file()` - CSV sampling (v1.0.1: resolves paths)
- `_sample_json_file()` - JSON/JSONL sampling (v1.0.1: resolves paths)

## Configuration File Format

### YAML Schema

```yaml
ai_coding_env: str  # 'chat', 'ide', 'agentic', 'cli'
max_file_size_mb: float

whitelist:
  files: List[str]        # Glob patterns
  directories: List[str]  # Glob patterns

blacklist:
  files: List[str]        # Glob patterns
  extensions: List[str]   # Leading dot normalized
  patterns: List[str]     # Python regex (use single quotes!)
  directories: List[str]  # Glob patterns

data_sampling:
  enabled: bool
  target_extensions: List[str]
  include_header: bool
  head_rows: int
  tail_rows: int
```

### Regex Pattern Guidelines

**CRITICAL:** Use **single quotes** for regex patterns in YAML:

```yaml
# ✅ CORRECT
patterns:
  - '_v\d{1,2}\.py$'
  - '^step\d{1,2}'
  - '^utils_'

# ❌ WRONG - causes YAML parsing error
patterns:
  - "_v\d{1,2}\.py$"  # ERROR: unknown escape character 'd'
```

## Running the Tool

### With uv (Recommended)
```bash
# From project root - now works with relative paths!
uv run python src/repo_distiller.py ../source ./dest [options]

# Development mode
uv sync --all-extras
```

### Without uv
```bash
# Activate virtual environment
python -m venv venv
source venv/bin/activate

# Install dependencies
pip install pyyaml

# Run
python src/repo_distiller.py ../source ./dest [options]
```

## Debugging Common Issues

### v1.0.1 Path Resolution

**Diagnostic Commands:**
```bash
# Test with absolute paths
uv run python src/repo_distiller.py   /absolute/path/to/source   /absolute/path/to/dest   --dry-run -v

# Test with relative paths (now works!)
uv run python src/repo_distiller.py   ../source-project   ./dest-project   --dry-run -v
```

**If you still see path errors:**
1. Check that source directory exists: `ls -la ../source-project`
2. Check permissions: `ls -ld ../source-project`
3. Verify you're running v1.0.1: `uv run python src/repo_distiller.py --version`

---

**Document Version**: 1.0.1  
**Last Updated**: 2025-12-14  
**Optimized for**: AI-assisted coding with uv + path resolution fix
</file>

<file path="docs/user-manual.md">
# Repository Distiller - User Manual

## Overview

Repository Distiller creates filtered copies of code repositories optimized for LLM context preparation.

**Version:** 1.0.1 (Path Resolution Fix)  
**Updated:** 2025-12-14

## What's New in v1.0.1

**FIXED:** Major bug causing "not in the subpath" errors resolved. The distiller now correctly handles both relative paths (like `../project`) and absolute paths by resolving all paths to absolute before processing.

## Installation

### Prerequisites
- Python 3.7 or higher
- uv package manager (recommended)

### Setup with uv
```bash
# Install uv if not already installed
curl -LsSf https://astral.sh/uv/install.sh | sh

# Clone or download the project
cd repo_distiller_project

# Install dependencies
uv sync

# Verify installation
uv run python src/repo_distiller.py --version
```

## Quick Start

### Basic Usage
```bash
# From project root with uv
# Now works with BOTH relative and absolute paths!
uv run python src/repo_distiller.py ../source-project ./dest-project
uv run python src/repo_distiller.py /absolute/path/source /absolute/path/dest

# Or without uv
python src/repo_distiller.py ../source ./dest
```

### Common Options

#### Dry Run (Preview Mode)
```bash
uv run python src/repo_distiller.py ../source ./dest --dry-run
```

#### Verbose Logging
```bash
uv run python src/repo_distiller.py ../source ./dest --verbose
```

#### Custom Configuration
```bash
uv run python src/repo_distiller.py ../source ./dest --config my_config.yaml
```

## Configuration

### YAML Configuration File

The distiller uses `config.yaml` for all filtering rules.

**IMPORTANT: Regex Pattern Syntax**

Regex patterns in YAML must use **single quotes** to avoid escaping issues:

```yaml
# ✅ CORRECT: Single quotes for regex patterns
blacklist:
  patterns:
    - '_v\d{1,2}\.py$'      # Matches: *_v1.py, *_v2.py, ..., *_v99.py
    - '^step\d{1,2}'         # Matches: step1_*, step2_*, etc.
    - '^utils_'               # Matches: utils_*.py

# ❌ WRONG: Double quotes cause YAML parsing errors
blacklist:
  patterns:
    - "_v\d{1,2}\.py$"      # ERROR: unknown escape character 'd'
```

### Configuration Sections

#### 1. Whitelist Rules (Highest Priority)

Only explicitly whitelisted files/directories are included (whitelist-only mode).

```yaml
whitelist:
  files:
    - "README.md"
    - "pyproject.toml"
    - "*.md"  # Glob pattern: all markdown files
  directories:
    - "src/"
    - "output/qa/step5_gold/"
```

#### 2. Blacklist Rules

Applied to files that ARE whitelisted for fine-grained exclusion.

```yaml
blacklist:
  files:
    - ".env"
    - ".gitignore"

  extensions:
    - ".png"
    - ".jpg"
    - ".pdf"

  patterns:  # Use single quotes!
    - '_v\d{1,2}\.py$'      # Versioned files
    - '^step\d{1,2}'         # Step files
    - '^utils_'               # Utility files

  directories:
    - ".git/"
    - "node_modules/"
    - "__pycache__/"
```

#### 3. Data Sampling

For large data files, sample head + tail instead of full copy:

```yaml
data_sampling:
  enabled: true
  target_extensions:
    - ".csv"
    - ".json"
    - ".jsonl"
  include_header: true  # For CSV/TSV
  head_rows: 5
  tail_rows: 5
```

## Regex Pattern Examples

### Filter Versioned Files

```yaml
patterns:
  - '_v\d{1,2}\.py$'  # Single quotes required!
```

**Matches:**
- `adjudicate_gold_v1.py` ✗
- `adjudicate_gold_v2.py` ✗
- `adjudicate_gold.py` ✓ (no version suffix)

### Filter Step-Numbered Files

```yaml
patterns:
  - '^step\d{1,2}'  # Single quotes required!
```

**Matches:**
- `step1_generate_examples.py` ✗
- `step2_filter_dataset.py` ✗
- `process_step1.py` ✓ (doesn't start with "step")

### Filter Utility Files

```yaml
patterns:
  - '^utils_'  # Single quotes required!
```

**Matches:**
- `utils_fix_annotator.py` ✗
- `utils_helper.py` ✗
- `data_utils.py` ✓ (doesn't start with "utils_")

## Troubleshooting

### v1.0.0 Error: "not in the subpath of"

**Problem:** In v1.0.0, using relative paths like `../project` caused errors.

**Solution:** Upgrade to v1.0.1 (this version). The path resolution issue is fixed.

### "Invalid YAML: unknown escape character"

**Problem:** Regex patterns use double quotes instead of single quotes.

**Solution:** Change regex patterns to use single quotes:

```yaml
# Before (causes error):
patterns:
  - "_v\d{1,2}\.py$"

# After (correct):
patterns:
  - '_v\d{1,2}\.py$'
```

### Files not being copied as expected

**Solution:** Run with `--verbose` and `--dry-run`:

```bash
uv run python src/repo_distiller.py ../source ./dest --dry-run --verbose
```

### Permission errors

**Solution:** Ensure you have read access to source and write access to destination.

## Output Summary

After completion, you'll see a summary:

```
======================================================================
DISTILLATION SUMMARY
======================================================================
Total files scanned:  156
Files copied:         42
Files sampled:        2
Files skipped:        112
Errors:               0

Skip reasons breakdown:
  blacklist_pattern:_v\d{1,2}\.py$  : 45
  blacklist_pattern:^step\d{1,2}     : 28
  blacklist_pattern:^utils_          : 12
  blacklist_ext:.md                   : 18
  not_whitelisted                     : 9
======================================================================
```

## Exit Codes

- `0`: Success
- `1`: Error occurred
- `130`: Operation cancelled by user (Ctrl+C)

## Best Practices

1. **Always use dry run first** for new configurations
2. **Use single quotes** for regex patterns in YAML
3. **Review the summary report** to verify expected files
4. **Check log files** in `./logs/` for detailed troubleshooting

## Support

For technical details, see `docs/tech-specs.md`.
</file>

<file path="config.yaml">
# Repository Distiller Configuration
# Customized for whitelist-only filtering approach
# For detailed documentation, see docs/user-manual.md

# AI Coding Environment: 'chat' (default), 'ide', 'agentic', 'cli'
ai_coding_env: 'chat'

# Maximum file size in megabytes (applied after whitelist check)
max_file_size_mb: 5

# ============================================================================
# WHITELIST RULES (Highest Priority - ONLY whitelisted items are included)
# ============================================================================
# Note: This configuration uses a whitelist-only approach.
# Only explicitly whitelisted files/directories will be copied.

whitelist:
  # Specific files to include (supports glob patterns with *)
  files:
    # Core project files
    - "README.md"
    - "pyproject.toml"
    - "config.yaml"

    # Data files to be sampled (head + tail)
    - "examples/openai/gpt-5-mini/gold-gpt5mini/project-13_anne-duncan-deflection-misdirection_-at-2025-12-07-21-15-b32f81aa.csv"
    - "examples/openai/gpt-5-mini/gold-gpt5mini/project-13_anne-duncan-deflection-misdirection_-at-2025-12-07-21-15-b32f81aa.json"

  # Directories to recursively include
  directories:
    - "output/qa/step5_gold/"
    - "src/"
    - "src/qa_gold_lib/"

# ============================================================================
# BLACKLIST RULES (Applied to whitelisted items - for fine-grained exclusion)
# ============================================================================
blacklist:
  # Specific files to exclude even if in whitelisted directory
  files:
    - ".env"
    - ".env.local"
    - ".gitignore"
    - ".DS_Store"
    - "package-lock.json"
    - "yarn.lock"
    - "poetry.lock"

  # File extensions to skip (within whitelisted directories)
  extensions:
    # Images and documents
    - ".png"
    - ".jpg"
    - ".jpeg"
    - ".pdf"
    - ".md"  # Markdown files excluded except README.md

    # Standard exclusions
    - ".log"
    - ".tmp"
    - ".cache"
    - ".pyc"
    - ".pyo"
    - ".pyd"
    - ".so"
    - ".dll"
    - ".exe"
    - ".bin"
    - ".o"
    - ".a"
    - ".lib"

    # Binary data formats
    - ".gif"
    - ".bmp"
    - ".ico"
    - ".svg"
    - ".mp4"
    - ".avi"
    - ".mov"
    - ".mp3"
    - ".wav"

    # Office documents
    - ".docx"
    - ".xlsx"
    - ".pptx"
    - ".doc"
    - ".xls"
    - ".ppt"

    # Archives
    - ".zip"
    - ".tar"
    - ".gz"
    - ".7z"
    - ".rar"

  # Python-compatible regex patterns for filename matching
  # These patterns exclude versioned files, step files, and utility files
  # IMPORTANT: Use single quotes for raw strings or escape backslashes
  patterns:
    # Versioned Python files: matches *_v1.py, *_v2.py, ..., *_v99.py
    - '_v\d{1,2}\.py$'

    # Step-numbered files: matches step1_*.py, step2_*.py, etc.
    - '^step\d{1,2}'

    # Utility files: matches utils_*.py
    - '^utils_'

    # Additional common patterns
    - '_backup\d*\.'
    - '\.bak$'
    - '~$'

  # Directories to exclude (even within whitelisted directories)
  directories:
    - ".git/"
    - ".svn/"
    - ".hg/"
    - "node_modules/"
    - "__pycache__/"
    - ".pytest_cache/"
    - ".mypy_cache/"
    - ".tox/"
    - "venv/"
    - "env/"
    - ".venv/"
    - ".uv/"
    - "build/"
    - "dist/"
    - "*.egg-info/"
    - "logs/"
    - "temp/"
    - "tmp/"
    - "cache/"
    - "coverage/"
    - ".coverage/"
    - "htmlcov/"

# ============================================================================
# DATA SAMPLING CONFIGURATION
# ============================================================================
# For large data files, copy only head + tail samples instead of full content
data_sampling:
  enabled: true

  # File extensions to sample (structured/semi-structured data)
  target_extensions:
    - ".csv"
    - ".tsv"
    - ".json"
    - ".jsonl"

  # For CSV/TSV: include header row (true recommended)
  include_header: true

  # Number of rows/objects to include from the beginning
  head_rows: 5

  # Number of rows/objects to include from the end
  tail_rows: 5
</file>

<file path="README.md">
# Repository Distiller

**Version:** 1.0.1 (Path Resolution Fix)  
**License:** MIT

## Overview

Repository Distiller is a command-line Python utility that creates intelligent, filtered copies of code repositories optimized for providing context to Large Language Models (LLMs) in AI-assisted coding workflows.

## Latest Update (v1.0.1)

**FIXED:** Path resolution issue causing "not in the subpath" errors when using relative paths like `../project-name`. All paths are now properly resolved to absolute paths before processing.

## Features

- ✅ **Smart Filtering**: Configurable whitelist/blacklist rules with glob pattern support
- ✅ **Whitelist-Only Mode**: Default blacklist-everything approach with explicit whitelisting
- ✅ **Data Sampling**: Automatically sample large CSV/JSON/JSONL files (head + tail)
- ✅ **Regex Patterns**: Filter versioned files, step-numbered scripts, and utility files
- ✅ **Size Control**: Skip files over configurable size threshold
- ✅ **Flexible Configuration**: YAML-based configuration with pattern matching
- ✅ **Dry Run Mode**: Preview actions before executing
- ✅ **Comprehensive Logging**: Dual-stream logging (console + file) with detailed statistics
- ✅ **AI-Optimized**: Designed specifically for LLM context preparation
- ✅ **Path Resolution**: Handles both relative and absolute paths correctly

## Quick Start

### Installation

```bash
# Clone or download this repository
cd repo_distiller_project

# Install with uv (recommended)
uv sync

# Or install dependencies manually
uv add pyyaml

# Verify installation
uv run python src/repo_distiller.py --version
```

### Basic Usage

```bash
# Distill a repository (from project root)
# Works with both relative and absolute paths
uv run python src/repo_distiller.py /path/to/source /path/to/destination
uv run python src/repo_distiller.py ../my-project ./distilled-project

# Dry run (preview only)
uv run python src/repo_distiller.py ../source ./dest --dry-run

# Verbose logging
uv run python src/repo_distiller.py ../source ./dest --verbose

# Custom configuration
uv run python src/repo_distiller.py ../source ./dest --config my_config.yaml
```

## Changelog

### v1.0.1 (2025-12-14)
- **FIXED:** Path resolution issue causing "not in the subpath" errors
- All paths now properly resolved to absolute paths using `.resolve()`
- Improved error handling for relative paths
- Enhanced logging with better path display

### v1.0.0 (2025-12-14)
- Initial release
- Core filtering engine with whitelist/blacklist support
- Whitelist-only mode for secure filtering
- Data sampling for CSV/JSON/JSONL files
- Regex pattern matching for versioned files
- Comprehensive logging and statistics
- Dry run mode
- YAML configuration
- uv package manager support

## Configuration Example

This project uses a **whitelist-only approach** by default. Edit `config.yaml` to customize:

```yaml
# AI coding environment: 'chat', 'ide', 'agentic', 'cli'
ai_coding_env: 'chat'

# Maximum file size in MB (applied after whitelist)
max_file_size_mb: 5

# WHITELIST: Only these are included
whitelist:
  files:
    - "README.md"
    - "pyproject.toml"
    - "config.yaml"
  directories:
    - "output/qa/step5_gold/"
    - "src/"
    - "src/qa_gold_lib/"

# BLACKLIST: Exclude from whitelisted items
blacklist:
  extensions:
    - ".png"
    - ".jpg"
    - ".pdf"
    - ".md"  # Except README.md (explicitly whitelisted)

  patterns:
    - '_v\d{1,2}\.py$'      # Excludes: *_v1.py, *_v2.py, ..., *_v99.py
    - '^step\d{1,2}'         # Excludes: step1_*, step2_*, ...
    - '^utils_'               # Excludes: utils_*.py

# Data sampling
data_sampling:
  enabled: true
  target_extensions: [".csv", ".json", ".jsonl"]
  head_rows: 5
  tail_rows: 5
```

## Documentation

- **[User Manual](docs/user-manual.md)** - Complete usage guide, configuration reference, and troubleshooting
- **[Technical Specification](docs/tech-specs.md)** - Architecture, API documentation, and extension guide

## Requirements

- Python 3.7+
- uv package manager (recommended)
- PyYAML (`uv add pyyaml`)

## License

MIT License - See LICENSE file for details.
</file>

</files>
